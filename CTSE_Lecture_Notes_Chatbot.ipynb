{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46faa67-377d-4834-8090-760a47e42059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\python311\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-huggingface in c:\\python311\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\python311\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: chromadb in c:\\python311\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: pdfminer.six in c:\\python311\\lib\\site-packages (20250506)\n",
      "Requirement already satisfied: faiss-cpu in c:\\python311\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: transformers in c:\\python311\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in c:\\python311\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: accelerate in c:\\python311\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (4.65.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: tiktoken in c:\\python311\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\python311\\lib\\site-packages (from langchain) (0.3.59)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\python311\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\python311\\lib\\site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\python311\\lib\\site-packages (from langchain) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\python311\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\python311\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\python311\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\python311\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.2 in c:\\python311\\lib\\site-packages (from langchain-huggingface) (0.31.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python311\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\python311\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: Pillow in c:\\python311\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\python311\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\python311\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: fastapi==0.115.9 in c:\\python311\\lib\\site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\python311\\lib\\site-packages (from chromadb) (0.34.2)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\python311\\lib\\site-packages (from chromadb) (2.1.3)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\python311\\lib\\site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\python311\\lib\\site-packages (from chromadb) (1.21.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\python311\\lib\\site-packages (from chromadb) (1.33.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\python311\\lib\\site-packages (from chromadb) (1.33.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\python311\\lib\\site-packages (from chromadb) (0.54b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\python311\\lib\\site-packages (from chromadb) (1.33.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\python311\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\python311\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\python311\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\python311\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\python311\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\python311\\lib\\site-packages (from chromadb) (0.15.3)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\python311\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\python311\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\python311\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\python311\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\python311\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\python311\\lib\\site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\python311\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\python311\\lib\\site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\python311\\lib\\site-packages (from pdfminer.six) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\python311\\lib\\site-packages (from pdfminer.six) (44.0.3)\n",
      "Requirement already satisfied: packaging in c:\\python311\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python311\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\python311\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\python311\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python311\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: psutil in c:\\python311\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\python311\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\python311\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: anyio in c:\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\python311\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\python311\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\python311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\python311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\python311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\python311\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.23.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\python311\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\python311\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\python311\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\python311\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\python311\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\python311\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.0 in c:\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.33.0 in c:\\python311\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.33.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.54b0 in c:\\python311\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.54b0 in c:\\python311\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b0 in c:\\python311\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.54b0 in c:\\python311\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.54b0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\python311\\lib\\site-packages (from opentelemetry-instrumentation==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\python311\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\python311\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\python311\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\python311\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python311\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python311\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\python311\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\python311\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\python311\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: pycparser in c:\\python311\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python311\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\python311\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\python311\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\python311\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\python311\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\python311\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Installing collected packages: tqdm\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Rolling back uninstall of tqdm\n",
      "  Moving to c:\\python311\\lib\\site-packages\\tqdm-4.65.0.dist-info\\\n",
      "   from C:\\Python311\\Lib\\site-packages\\~qdm-4.65.0.dist-info\n",
      "  Moving to c:\\python311\\lib\\site-packages\\tqdm\\\n",
      "   from C:\\Python311\\Lib\\site-packages\\~qdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'C:\\\\Python311\\\\Scripts\\\\tqdm.exe' -> 'C:\\\\Python311\\\\Scripts\\\\tqdm.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.4 (tags/v3.11.4:d2340ef, Jun  7 2023, 05:45:37) [MSC v.1934 64 bit (AMD64)]\n",
      "All libraries imported successfully!\n",
      "No GPU available, using CPU. This may be slower but will still work.\n",
      "Found 16 PDF files in the data directory:\n",
      " - data\\AWS User Groups Colombo - Introduction to AWS Cloud Platform.pdf\n",
      " - data\\CAP Theorem.pdf\n",
      " - data\\Cloud Computing 101.pdf\n",
      " - data\\Cloud Design Patterns - 1.pdf\n",
      " - data\\Cloud Design Patterns - 2.pdf\n",
      " - data\\cloud-computing-concepts-technology-amp-architecture-by-thomas-erl.pdf\n",
      " - data\\Containers 101 (1).pdf\n",
      " - data\\Intro to DevOps and Beyond (2).pdf\n",
      " - data\\Introduction to Microservices.pdf\n",
      " - data\\Key Essentials for Building Application in Cloud.pdf\n",
      " - data\\Lecture 01-Introduction to AI ML - Updated(2025).pdf\n",
      " - data\\Lecture 2 - Part 1.pdf\n",
      " - data\\Lecture 2 - Part 2.pdf\n",
      " - data\\Microservice Design Patterns.pdf\n",
      " - data\\ML Lec 2 - Part 1.pdf\n",
      " - data\\ML Lec 2 - Part 2 LLM.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 13:48:15 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 966 documents in 17.99 seconds\n",
      "Successfully loaded 0 documents in 0.00 seconds\n",
      "Total documents loaded: 966\n",
      "\n",
      "Sample document content:\n",
      "Document from data\\AWS User Groups Colombo - Introduction to AWS Cloud Platform.pdf - Page 0:\n",
      "Introduction to the AWS Cloud \n",
      "Platform\n",
      "Ravindu Nirmal Fernando\n",
      "2x AWS Community Builder | STL @ Sysco LABS\n",
      "Splitting documents into chunks...\n",
      "Split 966 documents into 1314 chunks in 0.03 seconds\n",
      "\n",
      "Sample chunk:\n",
      "Chunk from data\\AWS User Groups Colombo - Introduction to AWS Cloud Platform.pdf - Page 0:\n",
      "Introduction to the AWS Cloud \n",
      "Platform\n",
      "Ravindu Nirmal Fernando\n",
      "2x AWS Community Builder | STL @ Sysco LABS\n",
      "Loading embedding model...\n",
      "Embedding model loaded successfully in 4.90 seconds!\n",
      "Creating vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 13:48:42 - INFO - Loading faiss with AVX512 support.\n",
      "2025-05-12 13:48:42 - INFO - Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "2025-05-12 13:48:42 - INFO - Loading faiss with AVX2 support.\n",
      "2025-05-12 13:48:42 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-05-12 13:48:42 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully with 1314 chunks in 21.38 seconds!\n",
      "Vector store saved to ./vectorstore\n",
      "Testing retriever with query: 'What are the main topics of CTSE?'\n",
      "Retrieved 5 relevant chunks in 0.04 seconds\n",
      "\n",
      "Sample retrieved chunk:\n",
      "From data\\cloud-computing-concepts-technology-amp-architecture-by-thomas-erl.pdf - Page 151:\n",
      "resources.\n",
      "Loading model: distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully in 1.82 seconds!\n",
      "Successfully configured LLM with model: distilgpt2\n",
      "QA Chain created successfully!\n",
      "\n",
      "\n",
      "Question: What are the key aspects of agile development?\n",
      "\n",
      "Answer: \n",
      "You are an intelligent, helpful teaching assistant specializing in Current Trends in Software Engineering (CTSE).\n",
      "Your job is to provide accurate, informative answers to questions about CTSE topics based on the lecture notes.\n",
      "\n",
      "CONTEXT INFORMATION:\n",
      "Developers\n",
      "Focused on Agility\n",
      "Operators\n",
      "Focused on Stability\n",
      "\n",
      "This\tsection\tprovides\tinformation\tthat\tspecifies\tthe\tdetails\tof\tthe\tbusiness\tcase,\n",
      "such\tas\tthe\tfollowing:\n",
      "â€¢\t\n",
      "Business\tCase\tName\n",
      "â€¢\t\n",
      "Description\n",
      "\tâ€“\tA\tbrief\tsummary\tof\tthe\tbusiness\tcaseâ€™s\tpurpose\tand\tgoals.\n",
      "â€¢\t\n",
      "Sponsor\n",
      "\tâ€“\tIdentification\tof\tbusiness\tcase\tstakeholders.\n",
      "â€¢\t\n",
      "List\tof\tRevisions\t(optional)\n",
      "\tâ€“\tRevisions\tby\tdate,\tauthor,\tand\tapproval\tif\n",
      "control\tor\thistorical\tlogging\tis\trequired.\n",
      "G.2.\tBusiness\tNeeds\n",
      "The\texpected\tbenefits\tand\trequirements\tthat\tare\tto\tbe\taddressed\tand\tfulfilled\tby\n",
      "cloud\tadoption\tare\tdetailed\tin\tthis\tpart\tof\tthe\ttemplate:\n",
      "â€¢\t\n",
      "Background\n",
      "\tâ€“\tA\tdescription\tof\trelevant\thistorical\tinformation\tthat\tspurred\n",
      "on\tthe\tmotivation\tfor\tthe\tbusiness\tcase.\n",
      "â€¢\t\n",
      "Business\tGoals\n",
      "\tâ€“\tA\tlist\tof\tthe\ttactical\tand\tstrategic\tbusiness\tobjectives\tthat\n",
      "are\tassociated\twith\tthe\tbusiness\tcase.\n",
      "â€¢\t\n",
      "Business\tRequirements\n",
      "\tâ€“\tA\tlist\tof\tthe\tbusiness\trequirements\tthat\tare\n",
      "expected\tto\tbe\tfulfilled\tby\tthe\tachievement\tof\tthe\tbusiness\tgoals.\n",
      "â€¢\t\n",
      "Performance\tObjectives\n",
      "\tâ€“\tA\tlist\tof\tany\trelevant\tperformance\tobjectives\n",
      "\n",
      "DevOps Tools \n",
      "and \n",
      "Technologies\n",
      "\n",
      "Design Patterns \n",
      "A generally reusable solution to a recurring \n",
      "problem \n",
      "A template to solve the problem \n",
      "Best practices in approaching the problem \n",
      "Improve developer communication\n",
      "\n",
      "Intro to DevOps and Beyond\n",
      "Ravindu Nirmal Fernando\n",
      "\n",
      "QUESTION: What are the key aspects of agile development?\n",
      "\n",
      "INSTRUCTIONS:\n",
      "1. Answer the question using ONLY the information provided in the context.\n",
      "2. If the context doesn't contain enough information to fully answer the question, say so clearly.\n",
      "3. Keep your answer focused, clear, and educational.\n",
      "4. If appropriate, use bullet points or numbered lists to organize information.\n",
      "5. If the question involves code, provide examples when possible.\n",
      "6. Do not make up information or cite sources not present in the context.\n",
      "\n",
      "YOUR ANSWER:\n",
      "Â· The following words will be used for each topic you have written before; if they do not exist then it may appear as part of this article but only after reading them from within their source material. You can also add additional quotes here, which should help clarify what exactly these terms mean by adding more citations at any time!\n",
      "\n",
      "Sources: From 'Intro to DevOps and Beyond (2).pdf', Page 3, From 'cloud-computing-concepts-technology-amp-architecture-by-thomas-erl.pdf', Page 513, From 'Intro to DevOps and Beyond (2).pdf', Page 11, From 'Cloud Design Patterns - 1.pdf', Page 1, From 'Intro to DevOps and Beyond (2).pdf', Page 0\n",
      "\n",
      "(Response generated in 3.98 seconds)\n",
      "\n",
      "\n",
      "Question: What is RAG in the context of LLMs?\n",
      "\n",
      "Answer: \n",
      "You are an intelligent, helpful teaching assistant specializing in Current Trends in Software Engineering (CTSE).\n",
      "Your job is to provide accurate, informative answers to questions about CTSE topics based on the lecture notes.\n",
      "\n",
      "CONTEXT INFORMATION:\n",
      "Why Think of LLMs as a Toolkit?\n",
      "LLMs are not magic : They require crafted prompts, proper\n",
      "infrastructure, and optimization for reliable operation. Raw models\n",
      "alone are not enough.\n",
      "Building applications: Real-world LLM apps combine models,\n",
      "retrieval systems (RAG), external tools, and pipelines.\n",
      "Practical engineering is crucial : Success depends on handling\n",
      "scaling, failures, latency, and integration challenges.\n",
      "Lecture 02 - Part 2 (Upgraded) LLM Development Toolkit April 21, 2025 3 / 23\n",
      "\n",
      "LLM Development Toolkit\n",
      "Lecture 02 - Part 2 (Upgraded)\n",
      "April 21, 2025\n",
      "Lecture 02 - Part 2 (Upgraded) LLM Development Toolkit April 21, 2025 1 / 23\n",
      "\n",
      "LLM Architecture Essentials\n",
      "Tokenizer: Breaks text into tokens (smaller units of words/symbols)\n",
      "for the model to process.\n",
      "Embedding Layer: Converts tokens into high-dimensional vectors\n",
      "carrying semantic meaning.\n",
      "Transformer Block: Core engine â€” uses self-attention to model\n",
      "relationships between words.\n",
      "Output Head: Maps model predictions back into human-readable\n",
      "text.\n",
      "Reference: Attention is All You Need\n",
      "Lecture 02 - Part 2 (Upgraded) LLM Development Toolkit April 21, 2025 9 / 23\n",
      "\n",
      "Components of an LLM Agent\n",
      "Memory: Stores prior actions, results, and conversation state.\n",
      "Tools: API access, databases, search engines, calculators.\n",
      "Planner: Decomposes goals into achievable sub-tasks.\n",
      "Executor: Executes actions, monitors outputs, and feeds back into\n",
      "planning.\n",
      "Lecture 02 - Part 2 (Upgraded) LLM Development Toolkit April 21, 2025 18 / 23\n",
      "\n",
      "Setting Up LLMs: Practical Considerations\n",
      "Model Size Matters : 7B models need 8-16 GB RAM, 65B models\n",
      "need high-end GPUs.\n",
      "Quantization Helps: Shrinks models by using lower precision\n",
      "(int8/fp16) without major accuracy loss.\n",
      "Memory/Compute: Plan based on expected load â€” inference can\n",
      "be memory intensive.\n",
      "Latency Tradeoffs: Smaller models are faster but less powerful.\n",
      "Match model size to use case.\n",
      "Reference: Huggingface Quantization Guide\n",
      "Lecture 02 - Part 2 (Upgraded) LLM Development Toolkit April 21, 2025 12 / 23\n",
      "\n",
      "QUESTION: What is RAG in the context of LLMs?\n",
      "\n",
      "INSTRUCTIONS:\n",
      "1. Answer the question using ONLY the information provided in the context.\n",
      "2. If the context doesn't contain enough information to fully answer the question, say so clearly.\n",
      "3. Keep your answer focused, clear, and educational.\n",
      "4. If appropriate, use bullet points or numbered lists to organize information.\n",
      "5. If the question involves code, provide examples when possible.\n",
      "6. Do not make up information or cite sources not present in the context.\n",
      "\n",
      "YOUR ANSWER:\n",
      "The following list will be used only if you have any specific knowledge that would help us understand how we work with our software development team members. Please note that this may include some other things such like language support; documentation related issues relating to performance improvements; etc.; please do not assume there is no way out from here!\n",
      "\n",
      "Sources: From 'ML Lec 2 - Part 2 LLM.pdf', Page 2, From 'ML Lec 2 - Part 2 LLM.pdf', Page 0, From 'ML Lec 2 - Part 2 LLM.pdf', Page 8, From 'ML Lec 2 - Part 2 LLM.pdf', Page 17, From 'ML Lec 2 - Part 2 LLM.pdf', Page 11\n",
      "\n",
      "(Response generated in 2.42 seconds)\n",
      "\n",
      "\n",
      "Question: How can I implement CI/CD in a software project?\n",
      "\n",
      "Answer: \n",
      "You are an intelligent, helpful teaching assistant specializing in Current Trends in Software Engineering (CTSE).\n",
      "Your job is to provide accurate, informative answers to questions about CTSE topics based on the lecture notes.\n",
      "\n",
      "CONTEXT INFORMATION:\n",
      "becomes more complicated due to tightly coupled modules.\n",
      "â€¢ CI/CD Complications: Continuous integration and deployment become challenging as any \n",
      "update requires redeploying the entire application.\n",
      "â€¢ Vulnerability to System Failures: A bug in any module, like a memory leak, can crash the \n",
      "entire system.\n",
      "â€¢ Technological Rigidity: Adopting new frameworks or languages is costly and time-consuming, \n",
      "as it often requires rewriting the entire application.\n",
      "\n",
      "CI/ CD Management & Automation\n",
      "Writing Specifications and \n",
      "Documentation\n",
      "Infrastructure Management\n",
      "Cloud Deployment and \n",
      "Management\n",
      "Performance Assessment and \n",
      "Monitoring\n",
      "DevOps Engineer Role\n",
      "Assisting with DevOps culture \n",
      "apdotion\n",
      "\n",
      "Traditional deployment without GitOps:\n",
      "1 - A developer commits source code for the application.\n",
      "2 - A CI system builds the application and may also perform \n",
      "additional actions such as unit tests, security scans, static checks, \n",
      "etc.\n",
      "3 - The container image is stored in a Container registry.\n",
      "4 - The CI platform (or other external system) with direct access to \n",
      "the Kubernetes cluster creates a deployment using a variation of \n",
      "the â€œkubectl applyâ€ command.\n",
      "5 - The application is deployed on the cluster.\n",
      "\n",
      "â€¢ Continuous Integration (CI) - Software development practice where developers \n",
      "regularly merge their code changes into a central repository, after which automated \n",
      "builds and tests are run. \n",
      "â€¢ Continuous Delivery (CD) - Software development practice where code changes are \n",
      "automatically built, tested, and prepared for a release to production (automated \n",
      "code change deployment to staging/ pre-production system). \n",
      "â€¢ Continuous Deployment (CD) - Every change that passes all stages of the pipeline will \n",
      "be deployed into production (released to customers). This practice fully automates \n",
      "the whole release flow without human intervention and only a failed test will prevent a \n",
      "new change being deployed. \n",
      "â€¢ Microservices - The microservices architecture is a design approach to build a single \n",
      "application as a set of small services with each focusing on SRP. Each service can be \n",
      "created, deployed and run independently.\n",
      "\n",
      "â€œThe composition and integration of a set of processes, tools and automation \n",
      "(components) to build a coherent platform with the goal of empowering developers to \n",
      "be able to easily build, maintain and deploy their business logicâ€\n",
      "\n",
      "QUESTION: How can I implement CI/CD in a software project?\n",
      "\n",
      "INSTRUCTIONS:\n",
      "1. Answer the question using ONLY the information provided in the context.\n",
      "2. If the context doesn't contain enough information to fully answer the question, say so clearly.\n",
      "3. Keep your answer focused, clear, and educational.\n",
      "4. If appropriate, use bullet points or numbered lists to organize information.\n",
      "5. If the question involves code, provide examples when possible.\n",
      "6. Do not make up information or cite sources not present in the context.\n",
      "\n",
      "YOUR ANSWER:\n",
      "AUTHOR'S NOTE: You must have read this article before you begin writing anything else!\n",
      "\n",
      "Sources: From 'Introduction to Microservices.pdf', Page 5, From 'Intro to DevOps and Beyond (2).pdf', Page 22, From 'Lecture 2 - Part 2.pdf', Page 3, From 'Intro to DevOps and Beyond (2).pdf', Page 9, From 'Intro to DevOps and Beyond (2).pdf', Page 19\n",
      "\n",
      "(Response generated in 1.00 seconds)\n",
      "\n",
      "\n",
      "ðŸ“š CTSE Lecture Notes Chatbot\n",
      "Ask any question about Current Trends in Software Engineering\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f225b55c3142e186baa18ccef48877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Question:', layout=Layout(width='80%'), placeholder='Type your quesâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff5d19b523fb422aa0bf365293815e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid #ddd', border_left='1px solid #ddd', border_right='1px solid #ddâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To add new documents to the chatbot, use the following code:\n",
      "load_new_document('path/to/your/new/document.pdf')\n",
      "\n",
      "\n",
      "ðŸŽ‰ CTSE Chatbot is ready to use! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "# CTSE Lecture Notes Chatbot - Enhanced Implementation\n",
    "#\n",
    "# This notebook implements a chatbot that answers questions about CTSE lecture notes\n",
    "# using a Retrieval-Augmented Generation (RAG) approach with free LLMs.\n",
    "\n",
    "# Step 1: Install Required Libraries\n",
    "# Run this cell to install all the necessary packages\n",
    "# Note: This step only needs to be run once\n",
    "\n",
    "!pip install -U langchain langchain-huggingface sentence-transformers chromadb pdfminer.six faiss-cpu transformers torch accelerate tqdm tiktoken\n",
    "\n",
    "# Step 2: Import Required Libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Transformers for running local models\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Set up logging with timestamps\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(\"All libraries imported successfully!\")\n",
    "\n",
    "# Step 3: Check for GPU Availability and System Resources\n",
    "\n",
    "# Check for GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
    "    print(f\"GPU is available: {gpu_name} with {gpu_memory:.2f} GB memory\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU. This may be slower but will still work.\")\n",
    "\n",
    "# Step 4: Set Up Data Directory and Load PDF Files\n",
    "\n",
    "# Define data directory\n",
    "DATA_DIR = Path(\"./data\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Check for PDF files\n",
    "pdf_files = list(DATA_DIR.glob(\"**/*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDF files in the data directory:\")\n",
    "for file in pdf_files:\n",
    "    print(f\" - {file}\")\n",
    "\n",
    "if len(pdf_files) == 0:\n",
    "    print(\"\\nNo PDF files found! Please add your lecture notes to the data directory.\")\n",
    "    print(\"You can add them now and rerun this cell.\")\n",
    "    # Instead of stopping, we'll create a sample text file for testing if no PDFs exist\n",
    "    sample_file = DATA_DIR / \"sample_ctse_content.txt\"\n",
    "    if not sample_file.exists():\n",
    "        with open(sample_file, \"w\") as f:\n",
    "            f.write(\"\"\"\n",
    "            # Current Trends in Software Engineering (CTSE) - Sample Content\n",
    "            \n",
    "            ## Key Topics\n",
    "            \n",
    "            1. Agile Development Methodologies\n",
    "               - Scrum\n",
    "               - Kanban\n",
    "               - Extreme Programming (XP)\n",
    "               \n",
    "            2. DevOps Practices\n",
    "               - Continuous Integration (CI)\n",
    "               - Continuous Deployment (CD)\n",
    "               - Infrastructure as Code (IaC)\n",
    "               \n",
    "            3. Cloud Computing\n",
    "               - Software as a Service (SaaS)\n",
    "               - Platform as a Service (PaaS)\n",
    "               - Infrastructure as a Service (IaaS)\n",
    "               \n",
    "            4. Artificial Intelligence in Software Engineering\n",
    "               - Machine Learning Integration\n",
    "               - Automated Testing with AI\n",
    "               - RAG (Retrieval-Augmented Generation)\n",
    "            \"\"\")\n",
    "        print(f\"Created a sample text file at {sample_file} for testing purposes.\")\n",
    "\n",
    "# Step 5: Load Documents (PDF and Text files)\n",
    "\n",
    "# Set up document loaders\n",
    "loaders = [\n",
    "    DirectoryLoader(DATA_DIR, glob=\"**/*.pdf\", loader_cls=PyPDFLoader),\n",
    "    DirectoryLoader(DATA_DIR, glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "]\n",
    "\n",
    "# Load all documents\n",
    "documents = []\n",
    "for loader in loaders:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        docs = loader.load()\n",
    "        documents.extend(docs)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Successfully loaded {len(docs)} documents in {elapsed:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading documents with {loader.__class__.__name__}: {e}\")\n",
    "\n",
    "print(f\"Total documents loaded: {len(documents)}\")\n",
    "\n",
    "# Print a sample of document content to verify loading\n",
    "if documents:\n",
    "    print(\"\\nSample document content:\")\n",
    "    sample_doc = documents[0]\n",
    "    print(f\"Document from {sample_doc.metadata.get('source')} - Page {sample_doc.metadata.get('page', 'N/A')}:\")\n",
    "    print(sample_doc.page_content[:300] + \"...\" if len(sample_doc.page_content) > 300 else sample_doc.page_content)\n",
    "\n",
    "# Step 6: Process Documents for RAG - Split into Chunks\n",
    "\n",
    "# Split documents into smaller chunks with better parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try to split at paragraph boundaries first\n",
    ")\n",
    "\n",
    "# Split the documents with progress tracking\n",
    "print(\"Splitting documents into chunks...\")\n",
    "start_time = time.time()\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Split {len(documents)} documents into {len(chunks)} chunks in {elapsed:.2f} seconds\")\n",
    "\n",
    "# Print a sample chunk to verify splitting\n",
    "if chunks:\n",
    "    print(\"\\nSample chunk:\")\n",
    "    sample_chunk = chunks[0]\n",
    "    print(f\"Chunk from {sample_chunk.metadata.get('source')} - Page {sample_chunk.metadata.get('page', 'N/A')}:\")\n",
    "    print(sample_chunk.page_content[:300] + \"...\" if len(sample_chunk.page_content) > 300 else sample_chunk.page_content)\n",
    "\n",
    "# Step 7: Create Embeddings and Vector Store\n",
    "\n",
    "# Initialize embedding model - this is free and runs locally\n",
    "print(\"Loading embedding model...\")\n",
    "start_time = time.time()\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': str(device)}\n",
    ")\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Embedding model loaded successfully in {elapsed:.2f} seconds!\")\n",
    "\n",
    "# Create vector store\n",
    "print(\"Creating vector store...\")\n",
    "start_time = time.time()\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Vector store created successfully with {len(chunks)} chunks in {elapsed:.2f} seconds!\")\n",
    "\n",
    "# Save the vector store to disk for future use\n",
    "VECTORSTORE_PATH = \"./vectorstore\"\n",
    "vectorstore.save_local(VECTORSTORE_PATH)\n",
    "print(f\"Vector store saved to {VECTORSTORE_PATH}\")\n",
    "\n",
    "# Step 8: Set Up Retriever with Enhanced Parameters\n",
    "\n",
    "# Create retriever from vector store with better parameters\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance - better diversity in results\n",
    "    search_kwargs={\n",
    "        \"k\": 5,  # Retrieve top 5 chunks\n",
    "        \"fetch_k\": 10,  # Consider top 10 for diversity\n",
    "        \"lambda_mult\": 0.7  # Balance between relevance and diversity\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "if chunks:\n",
    "    test_query = \"What are the main topics of CTSE?\"\n",
    "    print(f\"Testing retriever with query: '{test_query}'\")\n",
    "    start_time = time.time()\n",
    "    retrieved_docs = retriever.invoke(test_query)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Retrieved {len(retrieved_docs)} relevant chunks in {elapsed:.2f} seconds\")\n",
    "    \n",
    "    if retrieved_docs:\n",
    "        print(\"\\nSample retrieved chunk:\")\n",
    "        sample_retrieved = retrieved_docs[0]\n",
    "        print(f\"From {sample_retrieved.metadata.get('source')} - Page {sample_retrieved.metadata.get('page', 'N/A')}:\")\n",
    "        print(sample_retrieved.page_content[:300] + \"...\" if len(sample_retrieved.page_content) > 300 else sample_retrieved.page_content)\n",
    "\n",
    "# Step 9: Load Language Model (LLM) with Error Handling\n",
    "\n",
    "def load_model(model_name, device, use_4bit=False):\n",
    "    \"\"\"Load a language model with error handling and fallbacks\"\"\"\n",
    "    try:\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Set quantization config if needed\n",
    "        bnb_config = None\n",
    "        if use_4bit and device.type == \"cuda\":\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "        \n",
    "        # Set model loading parameters\n",
    "        model_kwargs = {\n",
    "            \"torch_dtype\": torch.float16 if device.type == \"cuda\" else torch.float32,\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "            \"device_map\": \"auto\",\n",
    "            \"quantization_config\": bnb_config\n",
    "        }\n",
    "            \n",
    "        # Load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)\n",
    "            \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Model loaded successfully in {elapsed:.2f} seconds!\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model {model_name}: {e}\")\n",
    "        # Try fallback models if the primary one fails\n",
    "        fallback_models = [\n",
    "            \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "            \"distilgpt2\"\n",
    "        ]\n",
    "        \n",
    "        for fallback in fallback_models:\n",
    "            if fallback != model_name:\n",
    "                print(f\"Trying fallback model: {fallback}\")\n",
    "                try:\n",
    "                    return load_model(fallback, device, use_4bit=False)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to load fallback model {fallback}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        raise RuntimeError(f\"Failed to load any model: {e}\")\n",
    "\n",
    "# Try to load the model with a try/except block and fallback options\n",
    "try:\n",
    "    # Select appropriate model based on available hardware\n",
    "    if torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory > 8e9:  # >8GB VRAM\n",
    "        model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "        use_4bit = True\n",
    "    elif torch.cuda.is_available() and torch.cuda.get_device_properties(0).total_memory > 4e9:  # >4GB VRAM\n",
    "        model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "        use_4bit = False\n",
    "    else:\n",
    "        model_name = \"distilgpt2\"\n",
    "        use_4bit = False\n",
    "    \n",
    "    model, tokenizer = load_model(model_name, device, use_4bit)\n",
    "    \n",
    "    # Create text generation pipeline with appropriate parameters\n",
    "    text_generation_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.15,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Create LangChain wrapper\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "    print(f\"Successfully configured LLM with model: {model_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load any language model: {e}\")\n",
    "    print(\"\\nâš ï¸ ERROR: Could not load any language model. The chatbot will not be able to answer questions.\")\n",
    "    print(\"Please try installing additional libraries or using a smaller model.\")\n",
    "    # Create a dummy LLM for testing purposes\n",
    "    from langchain.llms.fake import FakeListLLM\n",
    "    llm = FakeListLLM(responses=[\"I'm sorry, I couldn't load a language model to answer your question.\"])\n",
    "    print(\"Created a dummy LLM for testing purposes.\")\n",
    "\n",
    "# Step 10: Create Custom Prompt Template for QA with Better Instructions\n",
    "\n",
    "# Define an improved prompt template that includes better context instructions\n",
    "template = \"\"\"\n",
    "You are an intelligent, helpful teaching assistant specializing in Current Trends in Software Engineering (CTSE).\n",
    "Your job is to provide accurate, informative answers to questions about CTSE topics based on the lecture notes.\n",
    "\n",
    "CONTEXT INFORMATION:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer the question using ONLY the information provided in the context.\n",
    "2. If the context doesn't contain enough information to fully answer the question, say so clearly.\n",
    "3. Keep your answer focused, clear, and educational.\n",
    "4. If appropriate, use bullet points or numbered lists to organize information.\n",
    "5. If the question involves code, provide examples when possible.\n",
    "6. Do not make up information or cite sources not present in the context.\n",
    "\n",
    "YOUR ANSWER:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Step 11: Create the QA Chain with Better Error Handling\n",
    "\n",
    "# Create the QA chain\n",
    "try:\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # 'stuff' simply stuffs all retrieved documents into the prompt\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True,  # Return source documents for reference\n",
    "        chain_type_kwargs={\"prompt\": PROMPT}\n",
    "    )\n",
    "    print(\"QA Chain created successfully!\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating QA chain: {e}\")\n",
    "    print(\"Failed to create QA chain. The chatbot will not be able to answer questions properly.\")\n",
    "\n",
    "# Step 12: Create Enhanced Chat Interface Function with Error Handling\n",
    "\n",
    "def ask_question(question, retriever=None, qa_chain=None):\n",
    "    \"\"\"\n",
    "    Ask a question to the CTSE chatbot with improved error handling and performance tracking.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask.\n",
    "        retriever: Optional retriever for direct document retrieval if qa_chain fails.\n",
    "        qa_chain: The QA chain to use.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The answer and source documents.\n",
    "    \"\"\"\n",
    "    if not question.strip():\n",
    "        return {\"answer\": \"Please ask a question about CTSE.\"}\n",
    "    \n",
    "    try:\n",
    "        # Track timing for performance analysis\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check if the question is 'answer' or similar edge cases\n",
    "        if question.lower() == 'answer':\n",
    "            raise ValueError(\"The query 'answer' is not valid.\")\n",
    "        \n",
    "        # Try to use the QA chain if available\n",
    "        if qa_chain is not None:\n",
    "            result = qa_chain.invoke({\"query\": question})\n",
    "            \n",
    "            # Format source information\n",
    "            sources = []\n",
    "            for doc in result.get(\"source_documents\", []):\n",
    "                source = f\"From '{os.path.basename(doc.metadata.get('source', 'unknown'))}'\"\n",
    "                if 'page' in doc.metadata:\n",
    "                    source += f\", Page {doc.metadata['page']}\"\n",
    "                if source not in sources:\n",
    "                    sources.append(source)\n",
    "            \n",
    "            # Add source information and timing to the answer\n",
    "            elapsed = time.time() - start_time\n",
    "            answer = result[\"result\"]\n",
    "            \n",
    "            if sources:\n",
    "                answer += \"\\n\\nSources: \" + \", \".join(sources)\n",
    "            \n",
    "            answer += f\"\\n\\n(Response generated in {elapsed:.2f} seconds)\"\n",
    "            \n",
    "            return {\"answer\": answer, \"sources\": sources, \"elapsed\": elapsed}\n",
    "        \n",
    "        # Fallback to retriever-only if QA chain isn't available\n",
    "        elif retriever is not None:\n",
    "            docs = retriever.invoke(question)\n",
    "            answer = \"I found these relevant excerpts from the lecture notes:\\n\\n\"\n",
    "            \n",
    "            for i, doc in enumerate(docs[:3], 1):  # Show top 3 results\n",
    "                source = f\"From '{os.path.basename(doc.metadata.get('source', 'unknown'))}'\"\n",
    "                if 'page' in doc.metadata:\n",
    "                    source += f\", Page {doc.metadata['page']}\"\n",
    "                    \n",
    "                answer += f\"--- Excerpt {i} ({source}) ---\\n\"\n",
    "                answer += doc.page_content.strip()[:300] + \"...\\n\\n\"\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            answer += f\"\\n(Documents retrieved in {elapsed:.2f} seconds)\"\n",
    "            \n",
    "            return {\"answer\": answer, \"elapsed\": elapsed}\n",
    "            \n",
    "        # No components available\n",
    "        else:\n",
    "            return {\"answer\": \"Sorry, neither the QA system nor document retriever is available.\"}\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing question: {e}\")\n",
    "        return {\"answer\": f\"Sorry, I encountered an error while processing your question: {str(e)}\"}\n",
    "\n",
    "\n",
    "# Step 13: Test the Chatbot\n",
    "\n",
    "# Test with some sample questions\n",
    "test_questions = [\n",
    "    \"What are the key aspects of agile development?\",\n",
    "    \"What is RAG in the context of LLMs?\",\n",
    "    \"How can I implement CI/CD in a software project?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n\\nQuestion: {question}\")\n",
    "    result = ask_question(question, retriever=retriever, qa_chain=qa_chain)\n",
    "    print(f\"\\nAnswer: {result['answer']}\")\n",
    "\n",
    "# Step 14: Interactive Chat Interface with History\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create widgets for the UI with improved styling\n",
    "question_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your question about CTSE here...',\n",
    "    description='Question:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Ask',\n",
    "    button_style='primary',\n",
    "    tooltip='Ask your question',\n",
    "    icon='question'\n",
    ")\n",
    "\n",
    "clear_button = widgets.Button(\n",
    "    description='Clear History',\n",
    "    button_style='warning',\n",
    "    tooltip='Clear chat history',\n",
    "    icon='trash'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        border='1px solid #ddd',\n",
    "        max_height='500px',\n",
    "        overflow='auto',\n",
    "        padding='10px'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add chat history management\n",
    "chat_history = []\n",
    "\n",
    "# Define the callback function for the button\n",
    "def on_submit_button_clicked(b):\n",
    "    question = question_input.value\n",
    "    if not question.strip():\n",
    "        return\n",
    "    \n",
    "    # Add question to history and clear input\n",
    "    chat_history.append((\"user\", question))\n",
    "    question_input.value = ''\n",
    "    \n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        \n",
    "        # Display chat history\n",
    "        for role, text in chat_history:\n",
    "            if role == \"user\":\n",
    "                print(f\"ðŸ§‘ You: {text}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"ðŸ¤– CTSE Bot: {text}\")\n",
    "                print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "        \n",
    "        # Get and display answer\n",
    "        print(\"ðŸ¤– CTSE Bot: Thinking...\")\n",
    "        \n",
    "    # Process question outside of output widget to avoid blocking UI\n",
    "    result = ask_question(question, retriever=retriever, qa_chain=qa_chain)\n",
    "    answer = result[\"answer\"]\n",
    "    \n",
    "    # Add answer to history\n",
    "    chat_history.append((\"bot\", answer))\n",
    "    \n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        \n",
    "        # Display updated chat history\n",
    "        for role, text in chat_history:\n",
    "            if role == \"user\":\n",
    "                print(f\"ðŸ§‘ You: {text}\")\n",
    "                print()\n",
    "            else:\n",
    "                print(f\"ðŸ¤– CTSE Bot: {text}\")\n",
    "                print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Clear history function\n",
    "def on_clear_button_clicked(b):\n",
    "    chat_history.clear()\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        print(\"Chat history cleared.\")\n",
    "\n",
    "# Connect the buttons to the callback functions\n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "clear_button.on_click(on_clear_button_clicked)\n",
    "\n",
    "# Display the UI with improved layout\n",
    "print(\"\\n\\nðŸ“š CTSE Lecture Notes Chatbot\")\n",
    "print(\"Ask any question about Current Trends in Software Engineering\")\n",
    "display(widgets.HBox([question_input, submit_button, clear_button]))\n",
    "display(output_area)\n",
    "\n",
    "with output_area:\n",
    "    print(\"Welcome to the CTSE Chatbot! Ask me any question about Current Trends in Software Engineering.\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Step 15: Add ability to load new documents without restarting\n",
    "\n",
    "def load_new_document(file_path):\n",
    "    \"\"\"\n",
    "    Load a new document into the system without restarting\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the new document\n",
    "    \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = Path(file_path)\n",
    "        if not path.exists():\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            return False\n",
    "            \n",
    "        print(f\"Loading new document: {path}\")\n",
    "        \n",
    "        # Choose appropriate loader based on file extension\n",
    "        if path.suffix.lower() == '.pdf':\n",
    "            loader = PyPDFLoader(str(path))\n",
    "        elif path.suffix.lower() == '.txt':\n",
    "            loader = TextLoader(str(path))\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {path.suffix}\")\n",
    "            return False\n",
    "            \n",
    "        # Load document\n",
    "        new_docs = loader.load()\n",
    "        print(f\"Loaded {len(new_docs)} pages/documents\")\n",
    "        \n",
    "        # Split into chunks\n",
    "        new_chunks = text_splitter.split_documents(new_docs)\n",
    "        print(f\"Split into {len(new_chunks)} chunks\")\n",
    "        \n",
    "        # Add to vector store\n",
    "        vectorstore.add_documents(new_chunks)\n",
    "        print(f\"Added to vector store\")\n",
    "        \n",
    "        # Save updated vector store\n",
    "        vectorstore.save_local(VECTORSTORE_PATH)\n",
    "        print(f\"Saved updated vector store\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading new document: {e}\")\n",
    "        print(f\"Failed to load document: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Create a function to show how to add new documents\n",
    "print(\"\\nTo add new documents to the chatbot, use the following code:\")\n",
    "print(\"load_new_document('path/to/your/new/document.pdf')\")\n",
    "\n",
    "print(\"\\n\\nðŸŽ‰ CTSE Chatbot is ready to use! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1cd661-3016-47b0-98d9-d985b6d9f304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
